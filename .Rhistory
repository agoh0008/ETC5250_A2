geom_point() +
geom_errorbar(aes(ymin=q2.5, ymax=q97.5), width=0.3) +
xlab("") + ylab("Coefficient") +
theme_bw() +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(size = 14)) +
labs(title = "PC1 Coefficients with Confidence Intervals")
# Chunk 5
summary_table <- PC1_boot_ci %>%
dplyr::select(var, q2.5, q5, q97.5, t0) %>%
mutate(across(where(is.numeric), ~round(., 3)))
column_alignment <- c("l", "c", "c", "c", "c")
kable(summary_table, format = "markdown",
caption = "Table 1: Summary of PC1 Coefficients with Confidence Intervals",
col.names = c("Variable", "2.5th Percentile", "Median", "97.5th Percentile", "Actual Value of Coefficient"),
align = column_alignment) %>%
kable_styling() %>%
scroll_box(width = "600px", height = "500px")
# Chunk 6
compute_PC2 <- function(data, index) {
pc2 <- prcomp(data[index,], center=TRUE, scale=TRUE)$rotation[,2]
if (sign(pc2[12]) < 0)
pc2 <- -pc2
return(pc2)
}
set.seed(201)
added_noise1 <- runif(54, -0.1, 0.1)
added_noise2 <- runif(54, -0.1, 0.1)
added_noise3 <- runif(54, -0.1, 0.1)
added_noise4 <- runif(54, -0.1, 0.1)
auswt20_adj <- auswt20 %>%
mutate(Hundreds = Hundreds + added_noise1,
Fifties = Fifties + added_noise2,
FourWickets = FourWickets + added_noise3,
FiveWickets = FiveWickets + added_noise4)
# Make sure sign of first PC element is positive
PC2_boot <- boot(data=auswt20_adj[,6:22], compute_PC2, R=1000)
colnames(PC2_boot$t) <- colnames(auswt20_adj[,6:22])
PC2_boot_ci <- as_tibble(PC2_boot$t) %>%
gather(var, coef) %>%
mutate(var = factor(var, levels=c("NotOuts", "Runs", "HighScore", "Average", "BallsFaced", "StrikeRate", "Hundreds", "Fifties", "Ducks", "Fours", "Sixes", "Overs", "Maidens", "Wickets", "Economy", "FourWickets", "FiveWickets"))) %>%
group_by(var) %>%
summarise(q2.5 = quantile(coef, 0.025),
q5 = median(coef),
q97.5 = quantile(coef, 0.975)) %>%
mutate(t0 = PC2_boot$t0)
# Chunk 7: PCA2
#| label: fig-2
#| fig-cap: Plot showing PC2 coefficients with confidence intervals.
ggplot(PC2_boot_ci, aes(x=var, y=t0)) +
geom_hline(yintercept=0, linetype=2, colour="red") +
geom_point() +
geom_errorbar(aes(ymin=q2.5, ymax=q97.5), width=0.3) +
xlab("") + ylab("Coefficient") +
theme_bw() +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
plot.title = element_text(size = 14)) +
labs(title = "PC2 Coefficients with Confidence Intervals")
# Chunk 8
summary_table <- PC2_boot_ci %>%
dplyr::select(var, q2.5, q5, q97.5, t0) %>%
mutate(across(where(is.numeric), ~round(., 3)))
column_alignment <- c("l", "c", "c", "c", "c")
kable(summary_table, format = "markdown",
caption = "Table 2: Summary of PC2 Coefficients with Confidence Intervals",
col.names = c("Variable", "2.5th Percentile", "Median", "97.5th Percentile", "Actual Value of Coefficient"),
align = column_alignment) %>%
kable_styling() %>%
scroll_box(width = "600px", height = "500px")
# Chunk 9
beta_0 <- 0.5
beta_1 <- -2
x <- -1
# Compute logit
logit_y <- beta_0 + beta_1 * x
# Compute logistic function
predicted_prob <- 1 / (1 + exp(-logit_y))
cat("The answer is:", round(predicted_prob, 3))
# Chunk 10
S <- matrix(c(2, 1, 1, 2), nrow = 2, byrow = TRUE)
x_a <- matrix(c(-2, 2))
x_b <- matrix(c(2, 2))
x_0 <- matrix(c(-3, -2))
# Inverse of the pooled variance-covariance matrix
S_inv <- solve(S)
# Mean vectors for each class
mean_a <- x_a
mean_b <- x_b
# Discriminant function values for each class
delta_a <- t(x_0) %*% S_inv %*% mean_a - 0.5 * t(mean_a) %*% S_inv %*% mean_a
delta_b <- t(x_0) %*% S_inv %*% mean_b - 0.5 * t(mean_b) %*% S_inv %*% mean_b
# Assign observation x_0 to class with the highest discriminant function value
if (delta_a > delta_b) {
class_predicted <- "A"
} else {
class_predicted <- "B"
}
cat("The class is: class", class_predicted)
# Chunk 11
S <- matrix(c(2, 1, 1, 2), nrow = 2)
x_a <- matrix(c(-2, 2), nrow = 2)
x_b <- matrix(c(2, 2), nrow = 2)
S_inv <- solve(S)
# Mean vector of class A and class B
mean_ab <- (x_a + x_b) / 2
# Observation x_0
x_0 <- c(-3, -2)
# Left-hand side of the inequality
lhs <- x_0 %*% S_inv %*% (x_a - x_b)
# Right-hand side of the inequality
rhs <- (t(mean_ab) %*% S_inv %*% (x_a - x_b))
# Predict class based on inequality
if (lhs > rhs) {
class_predicted <- "A"
} else {
class_predicted <- "B"
}
cat("The class is: class", class_predicted)
# Chunk 12
# Coordinates for Class A
orange_points <- matrix(c(-2, 4, -1, 1, 0, 0, 1, 1, 2, 4), ncol = 2, byrow = TRUE)
# Coordinates for Class B
blue_points <- matrix(c(-2, 6, -1, 3, 0, 2, 1, 3, 2, 6), ncol = 2, byrow = TRUE)
# Set up the PNG device for saving the plot
png("plot_output.png", width = 500, height = 500)
# Plot the points
plot(orange_points[,1], orange_points[,2], col = "orange", pch = 19, cex = 1,
xlim = c(-2, 2), ylim = c(0, 7),
xlab = "x1", ylab = "x2",
main = "Class Separation without using Logistic Regression or LDA",
cex.main = 1.3,
cex.lab = 1.5,
cex.axis = 1.5)
points(blue_points[,1], blue_points[,2], col = "blue", pch = 19, cex = 1)
legend("topright",
legend = c("Class A (Orange)", "Class B (Blue)"),
col = c("orange", "blue"),
pch = 19, cex = 1)
# Define parameters for the quadratic equation
a <- 1
b <- 0     # x-coordinate of the vertex
c <- 1.5   # y-coordinate of the vertex
# Generate x values
x_values <- seq(-2, 2, length.out = 100)
# Calculate corresponding y values using the quadratic equation
y_values <- a * (x_values - b)^2 + c
# Plot the quadratic separation rule
lines(x_values, y_values, col = "red", lty = 2)
text(-2, 1.3, "Quadratic Separation Rule: x2 = a(x1 - b)^2 + c", pos = 4, col = "red", cex = 1)
dev.off()
# Chunk 13
finance_and_birds <- read_csv("finance_and_birds.csv")
# Chunk 14
options(digits=2)
finance_and_birds_tidy <- finance_and_birds %>%
dplyr::select(type, trend, linearity, entropy, x_acf1) %>%
arrange(type) %>%
na.omit()
finance_and_birds_std <- finance_and_birds_tidy %>%
mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))
# Chunk 16: fig-3
#| label: fig-3
#| fig-cap: The scatterplot matrix above visualizes the relationships between variables related to financial time series and audio tracks of birds. Each point represents an observation, colored by class (i.e., type). The matrix helps assess assumptions for Linear Discriminant Analysis (LDA), including multivariate normality and homogeneity of covariance matrices.
ggscatmat(finance_and_birds_std, columns = 2:5,
color = "type") +
theme(legend.position = "none",
axis.text = element_blank()) +
theme_minimal() +
ggtitle("Scatterplot Matrix of Financial and Bird Data")
# Chunk 17: train-test
set.seed(1148)
finance_and_birds_split <- initial_split(finance_and_birds_std, 2/3,
strata = type)
finance_and_birds_tr <- training(finance_and_birds_split)
finance_and_birds_ts <- testing(finance_and_birds_split)
# Chunk 18: fit-lda
finance_and_birds_tr$type <- factor(finance_and_birds_tr$type)
lda_spec <- discrim_linear() %>%
set_mode("classification") %>%
set_engine("MASS", prior = c(0.5, 0.5))
lda_fit <- lda_spec %>%
fit(type ~ ., data = finance_and_birds_tr)
# Chunk 19
lda_fit
# Chunk 20: lda_confusion
finance_birds_tr_pred <- finance_and_birds_tr %>%
mutate(ptype = factor(predict(lda_fit$fit, finance_and_birds_tr)$class),
type = factor(type))
finance_birds_ts_pred <- finance_and_birds_ts %>%
mutate(ptype = factor(predict(lda_fit$fit, finance_and_birds_ts)$class),
type = factor(type))
train_table <- finance_birds_tr_pred %>%
count(type, ptype) %>%
group_by(type) %>%
mutate(cl_acc = n[ptype == type] / sum(n)) %>%
pivot_wider(names_from = ptype, values_from = n, values_fill = 0) %>%
dplyr::select(type, birdsongs, finance, cl_acc) %>%
kable(format = "html", caption = "Table 3: Training Set Prediction Results") %>%
kable_styling()
test_table <- finance_birds_ts_pred %>%
count(type, ptype) %>%
group_by(type) %>%
mutate(cl_acc = n[ptype == type] / sum(n)) %>%
pivot_wider(names_from = ptype, values_from = n, values_fill = 0) %>%
dplyr::select(type, birdsongs, finance, cl_acc) %>%
kable(format = "html", caption = "Table 4: Testing Set Prediction Results") %>%
kable_styling()
acc_tr <- accuracy(finance_birds_tr_pred, type, ptype)$.estimate
acc_test <- accuracy(finance_birds_ts_pred, type, ptype)$.estimate
train_table
test_table
cat("Training Classification Accuracy:", acc_tr)
cat("Test Classification Accuracy:", acc_test)
# Chunk 21: fit-logistic
log_reg <- logistic_reg() %>%
set_engine("glm") %>%
set_mode("classification")
log_fit <- log_reg %>%
fit(type ~ ., data = finance_and_birds_tr)
# Chunk 22
log_fit
# Chunk 23
finance_and_birds_tr$type <- factor(finance_and_birds_tr$type)
finance_and_birds_ts$type <- factor(finance_and_birds_ts$type)
finance_birds_tr_pred_log <- log_fit %>%
augment(new_data = finance_and_birds_tr) %>%
rename(ptype = .pred_class)
finance_birds_ts_pred_log <- log_fit %>%
augment(new_data = finance_and_birds_ts) %>%
rename(ptype = .pred_class)
# Chunk 24: logistic_confusion
train_table <- finance_birds_tr_pred_log %>%
count(type, ptype) %>%
group_by(type) %>%
mutate(cl_acc = n[ptype==type]/sum(n)) %>%
pivot_wider(names_from = ptype,
values_from = n, values_fill=0) %>%
dplyr::select(type, birdsongs, finance, cl_acc) %>%
kable(format = "html", caption = "Table 5: Training Set Prediction Results") %>%
kable_styling()
test_table <- finance_birds_ts_pred_log %>%
count(type, ptype) %>%
group_by(type) %>%
mutate(cl_acc = n[ptype==type]/sum(n)) %>%
pivot_wider(names_from = ptype,
values_from = n, values_fill=0) %>%
dplyr::select(type, birdsongs, finance, cl_acc) %>%
kable(format = "html", caption = "Table 6: Testing Set Prediction Results") %>%
kable_styling()
acc_tr_1 <- accuracy(finance_birds_tr_pred_log, type, ptype)$.estimate
acc_test_1 <- accuracy(finance_birds_ts_pred_log, type, ptype)$.estimate
train_table
test_table
cat("Training Classification Accuracy:", acc_tr_1)
cat("Test Classification Accuracy:", acc_test_1)
# Chunk 25
finance_birds_pred <- predict(lda_fit$fit, finance_and_birds_std)$x %>%
as.data.frame()
finance_birds_pred <- bind_cols(finance_and_birds_std, finance_birds_pred)
finance_birds_pred <- finance_birds_pred %>%
mutate(ptype = predict(lda_fit$fit, finance_and_birds_std)$class) %>%
mutate(set = ifelse(1:nrow(finance_and_birds_std) %in%
finance_and_birds_split$in_id,
"Train", "Test"),
err = ifelse(type != ptype, "Yes", "No"))
finance_birds_pred$set <- factor(finance_birds_pred$set, levels = c("Train", "Test"))
# Chunk 26
# Predictions for training data
finance_birds_tr_pred_log <- log_fit %>%
augment(new_data = finance_and_birds_tr) %>%
rename(ptype = .pred_class)
# Predictions for testing data
finance_birds_ts_pred_log <- log_fit %>%
augment(new_data = finance_and_birds_ts) %>%
rename(ptype = .pred_class)
# Combine predictions with original data
finance_birds_pred_log <- bind_rows(
finance_birds_tr_pred_log %>% mutate(set = "Train"),
finance_birds_ts_pred_log %>% mutate(set = "Test")
)
finance_birds_pred_log <- finance_birds_pred_log %>%
mutate(err = ifelse(type != ptype, "Yes", "No"))
# Chunk 27
finance_birds_pred <- mutate(finance_birds_pred, model = "LDA")
finance_birds_pred_log <- mutate(finance_birds_pred_log, model = "Logistic Regression")
finance_birds_pred_log$set <- factor(finance_birds_pred_log$set, levels = c("Train", "Test"))
finance_birds_pred <- rename(finance_birds_pred, lda_err = err)
finance_birds_pred_log <- rename(finance_birds_pred_log, log_err = err)
combined_data <- left_join(finance_birds_pred, finance_birds_pred_log,
by = c("trend", "linearity", "entropy", "x_acf1"))
# Chunk 28
# Plot for LDA model
plot_lda <- ggplot(combined_data, aes(x = LD1, y = 0,
color = type.x,
shape = lda_err)) +
geom_jitter(width = 0.2, height = 0.2) +
geom_rug(data = combined_data, aes(x = LD1), sides = "b", alpha = 0.5) +
scale_color_discrete(name = "Type") +
scale_shape_manual(name = "Error (LDA)", values = c("Yes" = 16, "No" = 1)) +
theme_minimal() +
labs(title = "Training and Test Data Representation in Discriminant Space (LDA Model)") +
facet_wrap(~set.x)
# Chunk 29
# Plot for logistic regression model
plot_logistic <- ggplot(combined_data, aes(x = LD1, y = 0,
color = type.y,
shape = log_err)) +
geom_jitter(width = 0.2, height = 0.2) +
geom_rug(data = combined_data, aes(x = LD1), sides = "b", alpha = 0.5) +
scale_color_brewer(palette = "Set1", name = "Type") +
scale_shape_manual(name = "Error (Logistic Regression)", values = c("Yes" = 16, "No" = 1)) +
theme_minimal() +
labs(title = "Training and Test Data Representation in Discriminant Space (Logistic Model)") +
facet_wrap(~set.y)
# Chunk 30: fig-4
#| label: fig-4
#| fig-cap: Scatterplot of Linear Discriminant Analysis (LDA) results. The plot displays the distribution of data points along LD1 axis with jitter, colored by true class type. The shape of points represents misclassifications (blank circles for correctly classified instances and full circles for misclassified instances). The data are separated into training and testing sets, aiding in visualizing the LDA model's performance.
plot_lda
# Chunk 31: fig-5
#| label: fig-5
#| fig-cap: Scatterplot of Logistic Regression Model results. The plot displays the distribution of data points along LD1 axis with jitter, colored by true class type. The shape of points represents misclassifications (blank circles for correctly classified instances and full circles for misclassified instances). The data are separated into training and testing sets, aiding in visualizing the Logistic Regression model's performance.
plot_logistic
# Chunk 32: fig-6
#| label: fig-6
#| fig-cap: The detour plot (left) showcases the multidimensional projection of time series data for both 'finance' and 'birdsongs' from the LDA model, color-coded by their true class. The confusion matrix (right) illustrates the classification results, with colors indicating the true class of the time series types. Notably, the bottom-right corner highlights misclassified 'birdsongs' observations, which were falsely predicted as 'finance'. The top-left corner indicates misclassified `finance` observations, which were falsely predicted as `birdsongs`.
finance_and_birds_std$type <- as.factor(finance_and_birds_std$type)
p_cl <- finance_and_birds_std %>%
mutate(p_type = predict(lda_fit$fit, finance_and_birds_std)$class) %>%
dplyr::select(trend:x_acf1, type, p_type)  %>%
mutate(true_class = jitter(as.numeric(type)),
predicted_class = jitter(as.numeric(p_type)))
p_cl_shared <- SharedData$new(p_cl)
detour_plot <- detour(p_cl_shared, tour_aes(
projection = trend:x_acf1,
colour = type)) %>%
tour_path(grand_tour(2),
max_bases=50, fps = 60) %>%
show_scatter(alpha = 0.9, axes = FALSE,
width = "100%", height = "450px")
conf_mat <- plot_ly(p_cl_shared,
x = ~predicted_class,
y = ~true_class,
color = ~type,
colors = viridis_pal(option = "D")(3),
height = 450,
width = 550) %>%
highlight(on = "plotly_selected",
off = "plotly_doubleclick") %>%
add_trace(type = "scatter",
mode = "markers")
bscols(
detour_plot, conf_mat,
widths = c(6, 6)
)
# Chunk 33: fig-7
#| label: fig-7
#| fig-cap: The detour plot (left) showcases the multidimensional projection of time series data for both 'finance' and 'birdsongs' from the logistic regresson model, color-coded by their true class. The confusion matrix (right) illustrates the classification results, with colors indicating the true class of the time series types. Notably, the bottom-right corner highlights misclassified 'birdsongs' observations, which were falsely predicted as 'finance'. The top-left corner indicates misclassified `finance` observations, which were falsely predicted as `birdsongs`.
p_cl_log <- finance_and_birds_std %>%
mutate(p_type = predict(log_fit,
finance_and_birds_std, type = "class")$.pred_class) %>%
dplyr::select(trend:x_acf1, type, p_type) %>%
mutate(true_class = jitter(as.numeric(type)),
predicted_class = jitter(as.numeric(p_type)))
p_cl_shared_log <- SharedData$new(p_cl_log)
detour_plot_log <- detour(p_cl_shared_log, tour_aes(
projection = trend:x_acf1,
colour = type)) %>%
tour_path(grand_tour(2),
max_bases=50, fps = 60) %>%
show_scatter(alpha = 0.9, axes = FALSE,
width = "100%", height = "450px")
conf_mat_log <- plot_ly(p_cl_shared_log,
x = ~predicted_class,
y = ~true_class,
color = ~type,
colors = viridis_pal(option = "D")(3),
height = 450,
width = 550) %>%
highlight(on = "plotly_selected",
off = "plotly_doubleclick") %>%
add_trace(type = "scatter",
mode = "markers")
bscols(
detour_plot_log, conf_mat_log,
widths = c(6, 6)
)
# Chunk 34: fig-8
#| label: fig-8
#| fig-cap: This figure illustrates the ROC curves for both Linear Discriminant Analysis (LDA) and logistic regression models based on the training dataset. The ROC curve for LDA is depicted in blue, while the curve for logistic regression is shown in red. The area under the curve (AUC) values for both models are provided in the legend, highlighting their respective performances in distinguishing 'birdsongs' as the positive class.
# Calculate predicted probabilities for LDA model on the training set
lda_probabilities <- predict(lda_fit$fit, finance_and_birds_tr, type = "prob")$posterior
# Extract probabilities for the positive class ("birdsongs")
lda_prob_birdsongs <- lda_probabilities[, "birdsongs"]
# Compute ROC curve for LDA model
roc_curve_lda <- roc(finance_and_birds_tr$type == "birdsongs", lda_prob_birdsongs)
# Calculate AUC for LDA model
auc_lda_score <- round(auc(roc_curve_lda), 3)
# Calculate predicted probabilities for logistic model on the training set
log_probabilities <- predict(log_fit, type = "prob", new_data = finance_and_birds_tr)
# Extract predicted probabilities for the positive class ("birdsongs")
log_prob_birdsongs <- log_probabilities$.pred_birdsongs
# Generate ROC curve for logistic model
roc_curve_logistic <- roc(finance_and_birds_tr$type == "birdsongs", log_prob_birdsongs)
# Calculate AUC for logistic model
auc_logistic_score <- round(auc(roc_curve_logistic), 3)
# Plot ROC curves for both models
plot(roc_curve_lda, col = "blue",
main = "ROC Curve - LDA vs Logistic Models (Train)",
xlab = "False Positive Rate",
ylab = "True Positive Rate")
plot(roc_curve_logistic, add = TRUE, col = "red")
# Add AUCs to the legend
legend("bottomright", legend = c(paste("LDA AUC =", auc_lda_score), paste("Logistic AUC =", auc_logistic_score)), col = c("blue", "red"), lty = 1, cex = 0.8)
# Chunk 35: fig-9
#| label: fig-9
#| fig-cap: This figure illustrates the ROC curves for both Linear Discriminant Analysis (LDA) and logistic regression models based on the test dataset. The ROC curve for LDA is depicted in blue, while the curve for logistic regression is shown in red. The area under the curve (AUC) values for both models are provided in the legend, highlighting their respective performances in distinguishing 'birdsongs' as the positive class.
# Calculate predicted probabilities for LDA model on the training set
lda_probabilities_ts <- predict(lda_fit$fit, finance_and_birds_ts, type = "prob")$posterior
# Extract probabilities for the positive class ("birdsongs")
lda_prob_birdsongs_ts <- lda_probabilities_ts[, "birdsongs"]
# Compute ROC curve for LDA model
roc_curve_lda_ts <- roc(finance_and_birds_ts$type == "birdsongs", lda_prob_birdsongs_ts)
# Calculate AUC for LDA model
auc_lda_score_ts <- round(auc(roc_curve_lda_ts), 3)
# Calculate predicted probabilities for logistic model on the training set
log_probabilities_ts <- predict(log_fit, type = "prob", new_data = finance_and_birds_ts)
# Extract predicted probabilities for the positive class ("birdsongs")
log_prob_birdsongs_ts <- log_probabilities_ts$.pred_birdsongs
# Generate ROC curve for logistic model
roc_curve_logistic_ts <- roc(finance_and_birds_ts$type == "birdsongs", log_prob_birdsongs_ts)
# Calculate AUC for logistic model
auc_logistic_score_ts <- round(auc(roc_curve_logistic_ts), 3)
# Plot ROC curves for both models
plot(roc_curve_lda_ts, col = "blue",
main = "ROC Curve - LDA vs Logistic Models (Test)",
xlab = "False Positive Rate",
ylab = "True Positive Rate")
plot(roc_curve_logistic_ts, add = TRUE, col = "red")
# Add AUCs to the legend
legend("bottomright", legend = c(paste("LDA AUC =", auc_lda_score_ts), paste("Logistic AUC =", auc_logistic_score_ts)), col = c("blue", "red"), lty = 1, cex = 0.8)
knitr::opts_chunk$set(
fig.width = 4,
fig.height = 4,
fig.align = "center",
out.width = "100%",
code.line.numbers = FALSE,
fig.retina = 4,
message = FALSE,
warning = FALSE,
cache = FALSE,
dev.args = list(pointsize = 11)
)
library(readr)
library(cricketdata)
library(boot)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(tidyr)
library(knitr)
library(kableExtra)
library(tourr)
library(tidymodels)
library(MASS)
library(discrim)
library(pROC)
library(viridis)
library(colorspace)
library(classifly)
library(detourr)
library(crosstalk)
library(plotly)
library(broom)
library(GGally)
auswt20 <- read_csv("https://raw.githubusercontent.com/numbats/iml/master/data/auswt20.csv")
library(readr)
library(cricketdata)
library(boot)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(tidyr)
library(knitr)
library(kableExtra)
library(tourr)
library(tidymodels)
library(MASS)
library(discrim)
library(pROC)
library(viridis)
library(colorspace)
library(classifly)
library(detourr)
library(crosstalk)
library(plotly)
library(broom)
library(GGally)
auswt20 <- read_csv("https://raw.githubusercontent.com/numbats/iml/master/data/auswt20.csv")
finance_and_birds <- read_csv("finance_and_birds.csv")
options(digits=2)
finance_and_birds_tidy <- finance_and_birds %>%
dplyr::select(type, trend, linearity, entropy, x_acf1) %>%
arrange(type) %>%
na.omit()
finance_and_birds_std <- finance_and_birds_tidy %>%
mutate_if(is.numeric, function(x) (x-mean(x))/sd(x))
set.seed(1148)
finance_and_birds_split <- initial_split(finance_and_birds_std, 2/3,
strata = type)
finance_and_birds_tr <- training(finance_and_birds_split)
finance_and_birds_ts <- testing(finance_and_birds_split)
str(finance_and_birds_tr)
